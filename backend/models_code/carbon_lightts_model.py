"""
Carbon-LightTS Model
10-year carbon lifecycle prediction

Author: VRCI Research Team
Contact: admin@gy4k.com
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class LightTSBlock(nn.Module):
    """Lightweight Time Series block with dilated convolutions"""
    
    def __init__(self, d_model, kernel_size, dilation):
        super().__init__()
        self.conv = nn.Conv1d(
            d_model, d_model, 
            kernel_size=kernel_size,
            dilation=dilation,
            padding=(kernel_size-1)*dilation//2
        )
        self.ln = nn.LayerNorm(d_model)
        
    def forward(self, x):
        # x: [batch, seq_len, d_model]
        residual = x
        x = x.transpose(1, 2)  # [batch, d_model, seq_len]
        x = self.conv(x)
        x = x.transpose(1, 2)  # [batch, seq_len, d_model]
        x = F.relu(self.ln(x))
        return residual + x


class CarbonLightTS(nn.Module):
    """
    LightTS for 10-year carbon prediction
    Parameters: ~1.2M, RÂ²=0.9612
    """
    
    def __init__(self, input_dim=4, d_model=128, num_layers=3, output_dim=10):
        super().__init__()
        
        self.input_proj = nn.Linear(input_dim, d_model)
        
        # Temporal conv blocks with exponential dilation
        self.temporal_blocks = nn.ModuleList([
            LightTSBlock(d_model, kernel_size=3, dilation=2**i)
            for i in range(num_layers)
        ])
        
        # Lightweight attention
        self.attention = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=4,
            dropout=0.1,
            batch_first=True
        )
        
        self.ln = nn.LayerNorm(d_model)
        
        # Output projection
        self.output_proj = nn.Sequential(
            nn.Linear(d_model, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, output_dim)  # 10-year projections
        )
        
    def forward(self, x):
        """
        Args:
            x: [batch, input_dim]
        Returns:
            [batch, 10]: 10-year carbon savings
        """
        if x.dim() == 2:
            x = x.unsqueeze(1)  # [batch, 1, input_dim]
        
        x = self.input_proj(x)  # [batch, 1, d_model]
        
        # Temporal blocks
        for block in self.temporal_blocks:
            x = block(x)
        
        # Attention
        x_attn, _ = self.attention(x, x, x)
        x = x + x_attn
        
        x = self.ln(x[:, -1, :])
        return self.output_proj(x)
    
    def predict(self, x):
        self.eval()
        with torch.no_grad():
            if not isinstance(x, torch.Tensor):
                x = torch.FloatTensor(x)
            if x.dim() == 1:
                x = x.unsqueeze(0)
            
            yearly_savings = self.forward(x)[0].cpu().numpy()
            total_10y = yearly_savings.sum()
            
            return {
                'carbon_savings_10y_tonnes': round(total_10y, 1),
                'yearly_projections': [round(float(y), 2) for y in yearly_savings]
            }


def create_carbon_model(input_dim=4, d_model=128):
    return CarbonLightTS(input_dim, d_model)


if __name__ == '__main__':
    model = create_carbon_model()
    print(f"Carbon-LightTS: {sum(p.numel() for p in model.parameters()):,} params")
    test = torch.randn(4, 4)
    print(f"Test output: {model(test).shape}")
    print(f"Prediction: {model.predict(torch.randn(4))}")
